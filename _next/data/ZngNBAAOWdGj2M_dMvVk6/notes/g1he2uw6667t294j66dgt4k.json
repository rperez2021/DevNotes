{"pageProps":{"note":{"id":"g1he2uw6667t294j66dgt4k","title":"Computer Vision","desc":"Notes on Azure AI Computer Vision","updated":1649876155601,"created":1648504410812,"custom":{},"fname":"devnotes.azure.ai.computer-vision","type":"note","vault":{"fsPath":"vault"},"contentHash":"c8cb49901d621a26f809090aca6daa3a","links":[],"anchors":{"computer-vision-models-and-capabilities":{"type":"header","text":"Computer Vision Models and Capabilities","value":"computer-vision-models-and-capabilities","line":7,"column":0,"depth":2},"computer-vision-services-in-microsoft-azure":{"type":"header","text":"Computer vision services in Microsoft Azure","value":"computer-vision-services-in-microsoft-azure","line":21,"column":0,"depth":2},"analyzing-images-with-the-cv-service":{"type":"header","text":"Analyzing images with the CV Service","value":"analyzing-images-with-the-cv-service","line":50,"column":0,"depth":2},"describing-an-image":{"type":"header","text":"Describing an image","value":"describing-an-image","line":52,"column":0,"depth":3},"tagging-visual-features":{"type":"header","text":"Tagging visual features","value":"tagging-visual-features","line":64,"column":0,"depth":3},"detecting-objects":{"type":"header","text":"Detecting objects","value":"detecting-objects","line":74,"column":0,"depth":3},"detecting-brands":{"type":"header","text":"Detecting brands","value":"detecting-brands","line":80,"column":0,"depth":3},"detecting-faces":{"type":"header","text":"Detecting faces","value":"detecting-faces","line":90,"column":0,"depth":3},"categorizing-an-image":{"type":"header","text":"Categorizing an image","value":"categorizing-an-image","line":98,"column":0,"depth":3},"computer-vision-86-catergory-taxonomy":{"type":"header","text":"Computer Vision 86 Catergory Taxonomy","value":"computer-vision-86-catergory-taxonomy","line":102,"column":0,"depth":3},"detecting-domain-specific-content":{"type":"header","text":"Detecting domain-specific content","value":"detecting-domain-specific-content","line":106,"column":0,"depth":3},"optical-character-recognition":{"type":"header","text":"Optical character recognition","value":"optical-character-recognition","line":115,"column":0,"depth":3},"additional-capabilities":{"type":"header","text":"Additional capabilities","value":"additional-capabilities","line":119,"column":0,"depth":3},"classifying-images-with-cv":{"type":"header","text":"Classifying Images with CV","value":"classifying-images-with-cv","line":128,"column":0,"depth":2},"model-evaluation":{"type":"header","text":"Model Evaluation","value":"model-evaluation","line":140,"column":0,"depth":3},"object-detection":{"type":"header","text":"Object Detection","value":"object-detection","line":148,"column":0,"depth":2},"object-detection-vs-image-classification":{"type":"header","text":"Object detection vs. image classification","value":"object-detection-vs-image-classification","line":152,"column":0,"depth":3},"image-tagging":{"type":"header","text":"Image Tagging","value":"image-tagging","line":156,"column":0,"depth":3},"model-training-and-evaluation":{"type":"header","text":"Model training and evaluation","value":"model-training-and-evaluation","line":160,"column":0,"depth":3},"face-detection":{"type":"header","text":"Face Detection","value":"face-detection","line":166,"column":0,"depth":2},"facial-analysis":{"type":"header","text":"Facial Analysis","value":"facial-analysis","line":172,"column":0,"depth":3},"facial-recognition":{"type":"header","text":"Facial recognition","value":"facial-recognition","line":180,"column":0,"depth":3},"use-cases-for-face-detection-and-analysis":{"type":"header","text":"Use Cases for Face Detection and Analysis","value":"use-cases-for-face-detection-and-analysis","line":184,"column":0,"depth":3},"text-detection":{"type":"header","text":"Text Detection","value":"text-detection","line":193,"column":0,"depth":2},"optical-character-recognition-ocr":{"type":"header","text":"Optical Character Recognition (OCR)","value":"optical-character-recognition-ocr","line":195,"column":0,"depth":3},"machine-reading-comprehension-mrc":{"type":"header","text":"Machine Reading Comprehension (MRC)","value":"machine-reading-comprehension-mrc","line":199,"column":0,"depth":3}},"children":[],"parent":"bxv1vlc2jzbzpchawt74rzq","data":{}},"body":"<h1 id=\"computer-vision\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Computer Vision</h1>\n<h2 id=\"computer-vision-models-and-capabilities\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision-models-and-capabilities\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Computer Vision Models and Capabilities</h2>\n<ul>\n<li>\n<p>Image Classification</p>\n</li>\n<li>\n<p>Object Detection</p>\n</li>\n<li>\n<p>Semantic Segmentation - Semantic segmentation is an advanced machine learning technique in which individual pixels in the image are classified according to the object to which they belong.</p>\n</li>\n<li>\n<p>Image Analysis - You can create solutions that combine machine learning models with advanced image analysis techniques to extract information from images, including \"tags\" that could help catalog the image or even descriptive captions that summarize the scene shown in the image.</p>\n</li>\n<li>\n<p>Face detection, analysis and recognition</p>\n</li>\n<li>\n<p>Optical Character Recognition - is a technique used to detect and read text in images. You can use OCR to read text in photographs (for example, road signs or store fronts) or to extract information from scanned documents such as letters, invoices, or forms.</p>\n</li>\n</ul>\n<h2 id=\"computer-vision-services-in-microsoft-azure\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision-services-in-microsoft-azure\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Computer vision services in Microsoft Azure</h2>\n<table aria-label=\"Computer vision services in Microsoft Azure\" class=\"table\">\n<thead>\n<tr>\n<th>Service</th>\n<th>Capabilities</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Computer Vision</strong></td>\n<td>You can use this service to analyze images and video, and extract descriptions, tags, objects, and text.</td>\n</tr>\n<tr>\n<td><strong>Custom Vision</strong></td>\n<td>Use this service to train custom image classification and object detection models using your own images.</td>\n</tr>\n<tr>\n<td><strong>Face</strong></td>\n<td>The Face service enables you to build face detection and facial recognition solutions.</td>\n</tr>\n<tr>\n<td><strong>Form Recognizer</strong></td>\n<td>Use this service to extract information from scanned forms and invoices.</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"analyzing-images-with-the-cv-service\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#analyzing-images-with-the-cv-service\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Analyzing images with the CV Service</h2>\n<h3 id=\"describing-an-image\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#describing-an-image\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Describing an image</h3>\n<p>Computer Vision has the ability to analyze an image, evaluate the objects that are detected, and generate a human-readable phrase or sentence that can describe what was detected in the image. Depending on the image contents, the service may return multiple results, or phrases. Each returned phrase will have an associated confidence score, indicating how confident the algorithm is in the supplied description. The highest confidence phrases will be listed first.</p>\n<p>To help you understand this concept, consider the following image of the Empire State building in New York. The returned phrases are listed below the image in the order of confidence.</p>\n<p><img src=\"/DevNotes/assets/black-white-buildings.png\" alt=\"NYC Skyline\"></p>\n<ul>\n<li>A black and white photo of a city</li>\n<li>A black and white photo of a large city</li>\n<li>A large white building in a city</li>\n</ul>\n<h3 id=\"tagging-visual-features\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#tagging-visual-features\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Tagging visual features</h3>\n<p>The image descriptions generated by Computer Vision are based on a set of thousands of recognizable objects, which can be used to suggest tags for the image. These tags can be associated with the image as metadata that summarizes attributes of the image; and can be particularly useful if you want to index an image along with a set of key terms that might be used to search for images with specific attributes or contents.</p>\n<p>For example, the tags returned for the Empire State building image include:</p>\n<ul>\n<li>skyscraper</li>\n<li>tower</li>\n<li>building</li>\n</ul>\n<h3 id=\"detecting-objects\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-objects\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Detecting objects</h3>\n<p>The object detection capability is similar to tagging, in that the service can identify common objects; but rather than tagging, or providing tags for the recognized objects only, this service can also return what is known as bounding box coordinates. Not only will you get the type of object, but you will also receive a set of coordinates that indicate the top, left, width, and height of the object detected, which you can use to identify the location of the object in the image, like this:</p>\n<p><img src=\"/DevNotes/assets/black-white-buildings-objects.png\" alt=\"NYC Skyline Objects Highlighted\"></p>\n<h3 id=\"detecting-brands\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-brands\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Detecting brands</h3>\n<p>This feature provides the ability to identify commercial brands. The service has an existing database of thousands of globally recognized logos from commercial brands of products.</p>\n<p>When you call the service and pass it an image, it performs a detection task and determine if any of the identified objects in the image are recognized brands. The service compares the brands against its database of popular brands spanning clothing, consumer electronics, and many more categories. If a known brand is detected, the service returns a response that contains the brand name, a confidence score (from 0 to 1 indicating how positive the identification is), and a bounding box (coordinates) for where in the image the detected brand was found.</p>\n<p>For example, in the following image, a laptop has a Microsoft logo on its lid, which is identified and located by the Computer Vision service.</p>\n<p><img src=\"/DevNotes/assets/laptop.png\" alt=\"Laptop Brand\"></p>\n<h3 id=\"detecting-faces\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-faces\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Detecting faces</h3>\n<p>The Computer Vision service can detect and analyze human faces in an image, including the ability to determine age and a bounding box rectangle for the location of the face(s). The facial analysis capabilities of the Computer Vision service are a subset of those provided by the dedicated Face Service. If you need basic face detection and analysis, combined with general image analysis capabilities, you can use the Computer Vision service; but for more comprehensive facial analysis and facial recognition functionality, use the Face service.</p>\n<p>The following example shows an image of a person with their face detected and approximate age estimated.</p>\n<p><img src=\"/DevNotes/assets/face.png\" alt=\"Face Recognition\"></p>\n<h3 id=\"categorizing-an-image\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#categorizing-an-image\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Categorizing an image</h3>\n<p>Computer Vision can categorize images based on their contents. The service uses a parent/child hierarchy with a \"current\" limited set of categories. When analyzing an image, detected objects are compared to the existing categories to determine the best way to provide the categorization. As an example, one of the parent categories is <code>people_</code>. This image of a person on a roof is assigned a category of <code>people_</code>.</p>\n<h3 id=\"computer-vision-86-catergory-taxonomy\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision-86-catergory-taxonomy\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Computer Vision 86 Catergory Taxonomy</h3>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/category-taxonomy\">Link to Microsoft Docs List</a></p>\n<h3 id=\"detecting-domain-specific-content\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-domain-specific-content\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Detecting domain-specific content</h3>\n<p>When categorizing an image, the Computer Vision service supports two specialized domain models:</p>\n<ul>\n<li><strong>Celebrities</strong> - The service includes a model that has been trained to identify thousands of well-known celebrities from the worlds of sports, entertainment, and business.</li>\n<li><strong>Landmarks</strong> - The service can identify famous landmarks, such as the Taj Mahal and the Statue of Liberty.</li>\n</ul>\n<p>For example, when analyzing the following image for landmarks, the Computer Vision service identifies the Eiffel Tower, with a confidence of 99.41%.</p>\n<h3 id=\"optical-character-recognition\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#optical-character-recognition\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Optical character recognition</h3>\n<p>The Computer Vision service can use optical character recognition (OCR) capabilities to detect printed and handwritten text in images. This capability is explored in the Read text with the Computer Vision service module on Microsoft Learn.</p>\n<h3 id=\"additional-capabilities\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#additional-capabilities\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Additional capabilities</h3>\n<p>In addition to these capabilities, the Computer Vision service can:</p>\n<ul>\n<li><strong>Detect image types</strong> - for example, identifying clip art images or line drawings.</li>\n<li><strong>Detect image color schemes</strong> - specifically, identifying the dominant foreground, background, and overall colors in an image.</li>\n<li><strong>Generate thumbnails</strong> - creating small versions of images.</li>\n<li><strong>Moderate content</strong> - detecting images that contain adult content or depict violent, gory scenes.</li>\n</ul>\n<h2 id=\"classifying-images-with-cv\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#classifying-images-with-cv\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Classifying Images with CV</h2>\n<p>Image classification is a machine learning technique in which the object being classified is an image, such as a photograph.</p>\n<p>To create an image classification model, you need data that consists of features and their labels. The existing data is a set of categorized images. Digital images are made up of an array of pixel values, and these are used as features to train the model based on the known image classes.</p>\n<p>You can use a machine learning classification technique to predict which category, or class, something belongs to. Classification machine learning models use a set of inputs, which we call features, to calculate a probability score for each possible class and predict a label that indicates the most likely class that an object belongs to.</p>\n<p>The model is trained to match the patterns in the pixel values to a set of class labels. After the model has been trained, you can use it with new sets of features to predict unknown label values.</p>\n<p>Most modern image classification solutions are based on deep learning techniques that make use of convolutional neural networks (CNNs) to uncover patterns in the pixels that correspond to particular classes. Training an effective CNN is a complex task that requires considerable expertise in data science and machine learning.</p>\n<h3 id=\"model-evaluation\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#model-evaluation\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Model Evaluation</h3>\n<p>Model training process is an iterative process in which the Custom Vision service repeatedly trains the model using some of the data, but holds some back to evaluate the model. At the end of the training process, the performance for the trained model is indicated by the following evaluation metrics:</p>\n<ul>\n<li><strong>Precision</strong>: What percentage of the class predictions made by the model were correct? For example, if the model predicted that 10 images are oranges, of which eight were actually oranges, then the precision is 0.8 (80%).</li>\n<li><strong>Recall</strong>: What percentage of class predictions did the model correctly identify? For example, if there are 10 images of apples, and the model found 7 of them, then the recall is 0.7 (70%).</li>\n<li><strong>Average Precision (AP)</strong>: An overall metric that takes into account both precision and recall).</li>\n</ul>\n<h2 id=\"object-detection\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#object-detection\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Object Detection</h2>\n<p>Object detection is a form of machine learning based computer vision in which a model is trained to recognize individual types of objects in an image, and to identify their location in the image.</p>\n<h3 id=\"object-detection-vs-image-classification\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#object-detection-vs-image-classification\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Object detection vs. image classification</h3>\n<p>Image classification is a machine learning based form of computer vision in which a model is trained to categorize images based on the primary subject matter they contain. Object detection goes further than this to classify individual objects within the image, and to return the coordinates of a bounding box that indicates the object's location.</p>\n<h3 id=\"image-tagging\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#image-tagging\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Image Tagging</h3>\n<p>Before you can train an object detection model, you must tag the classes and bounding box coordinates in a set of training images.</p>\n<h3 id=\"model-training-and-evaluation\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#model-training-and-evaluation\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Model training and evaluation</h3>\n<ul>\n<li><strong>Precision</strong>: What percentage of class predictions did the model correctly identify? For example, if the model predicted that 10 images are oranges, of which eight were actually oranges, then the precision is 0.8 (80%).</li>\n<li><strong>Recall</strong>: What percentage of the class predictions made by the model were correct? For example, if there are 10 images of apples, and the model found 7 of them, then the recall is 0.7 (70%).</li>\n<li><strong>Mean Average Precision (mAP)</strong>: An overall metric that takes into account both precision and recall across all classes).</li>\n</ul>\n<h2 id=\"face-detection\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#face-detection\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Face Detection</h2>\n<p>Face detection involves identifying regions of an image that contain a human face, typically by returning bounding box coordinates that form a rectangle around the face, like this:</p>\n<p><img src=\"/DevNotes/assets/face-detection.png\" alt=\"Face Detection\"></p>\n<h3 id=\"facial-analysis\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#facial-analysis\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Facial Analysis</h3>\n<p>Moving beyond simple face detection, some algorithms can also return other information, such as facial landmarks (nose, eyes, eyebrows, lips, and others).</p>\n<p><img src=\"/DevNotes/assets/facial-landmarks.png\" alt=\"Facial Landmarks\"></p>\n<p>hese facial landmarks can be used as features with which to train a machine learning model from which you can infer information about a person, such as their perceived age or perceived emotional state.</p>\n<h3 id=\"facial-recognition\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#facial-recognition\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Facial recognition</h3>\n<p>A further application of facial analysis is to train a machine learning model to identify known individuals from their facial features. This usage is more generally known as facial recognition, and involves using multiple images of each person you want to recognize to train a model so that it can detect those individuals in new images on which it wasn't trained.</p>\n<h3 id=\"use-cases-for-face-detection-and-analysis\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#use-cases-for-face-detection-and-analysis\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Use Cases for Face Detection and Analysis</h3>\n<ul>\n<li><strong>Security</strong> - facial recognition can be used in building security applications, and increasingly it is used in smart phones operating systems for unlocking devices.</li>\n<li><strong>Social media</strong> - facial recognition can be used to automatically tag known friends in photographs.</li>\n<li><strong>Intelligent monitoring</strong> - for example, an automobile might include a system that monitors the driver's face to determine if the driver is looking at the road, looking at a mobile device, or shows signs of tiredness.</li>\n<li><strong>Advertising</strong> - analyzing faces in an image can help direct advertisements to an appropriate demographic audience.</li>\n<li><strong>Missing persons</strong> - using public cameras systems, facial recognition can be used to identify if a missing person is in the image frame.</li>\n<li><strong>Identity validation</strong> - useful at ports of entry kiosks where a person holds a special entry permit.</li>\n</ul>\n<h2 id=\"text-detection\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#text-detection\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Text Detection</h2>\n<h3 id=\"optical-character-recognition-ocr\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#optical-character-recognition-ocr\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Optical Character Recognition (OCR)</h3>\n<p>The basic foundation of processing printed text is optical character recognition (OCR), in which a model can be trained to recognize individual shapes as letters, numerals, punctuation, or other elements of text. Much of the early work on implementing this kind of capability was performed by postal services to support automatic sorting of mail based on postal codes. Since then, the state-of-the-art for reading text has moved on, and it's now possible to build models that can detect printed or handwritten text in an image and read it line-by-line or even word-by-word.</p>\n<h3 id=\"machine-reading-comprehension-mrc\"><a aria-hidden=\"true\" class=\"anchor-heading\" href=\"#machine-reading-comprehension-mrc\"><svg aria-hidden=\"true\" viewBox=\"0 0 16 16\"><use xlink:href=\"#svg-link\"></use></svg></a>Machine Reading Comprehension (MRC)</h3>\n<p>At the other end of the scale, there is machine reading comprehension (MRC), in which an AI system not only reads the text characters, but can use a semantic model to interpret what the text is about.</p>","noteIndex":{"id":"root","title":"root","desc":"","updated":1605266684036,"created":1595961348801,"stub":false,"custom":{"stub":false,"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"cf4622cd4e7841f93fb3051e6203c8e6","links":[],"anchors":{},"children":["uRNDWirv66xgHSdrPwMcw","lcTujcWsxBfPEpzP49YbQ","FTV8hEHhyid0DiEvv9gNj","ph4ta0hoi7x0vch4uoz72hn","boufkx25rro7ysodpanan5g","01u0co3RYjOM1bjpc2qIU","xt9womgr90xi5ui5nezev7b","qpngd6ap92mmahr4bh4gc47","hobifg8vvmmg17kt2z5p36a"],"parent":null,"data":{},"body":"\nThis is the root for your Dendron vault.\n\nIf you decide to publish your entire vault, it will be your landing page. You are free to customize any part of this page except the frontmatter at the top, between the `---`. \n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"dendronVersion":"0.88.0","enableFullHierarchyNoteTitle":false,"enableHandlebarTemplates":true,"enableSmartRefs":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"assetsPrefix":"/DevNotes","copyAssets":true,"siteHierarchies":["root"],"enableSiteLastModified":true,"siteRootDir":"docs","siteUrl":"https://rperez2021.github.io","enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"writeStubs":false,"seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"root","enableTaskNotes":true}}},"__N_SSG":true}