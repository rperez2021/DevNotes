<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Computer Vision</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Notes on Azure AI Computer Vision"/><meta property="og:title" content="Computer Vision"/><meta property="og:description" content="Notes on Azure AI Computer Vision"/><meta property="og:url" content="https://rperez2021.github.io/DevNotes/notes/g1he2uw6667t294j66dgt4k/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="3/28/2022"/><meta property="article:modified_time" content="4/13/2022"/><link rel="canonical" href="https://rperez2021.github.io/DevNotes/notes/g1he2uw6667t294j66dgt4k/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/DevNotes/_next/static/css/eed8bf81abfd52c3.css" as="style"/><link rel="stylesheet" href="/DevNotes/_next/static/css/eed8bf81abfd52c3.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/DevNotes/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/DevNotes/_next/static/chunks/webpack-0c999df4017467fc.js" defer=""></script><script src="/DevNotes/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/DevNotes/_next/static/chunks/main-772151c4b16ecf54.js" defer=""></script><script src="/DevNotes/_next/static/chunks/pages/_app-12b82b1567467517.js" defer=""></script><script src="/DevNotes/_next/static/chunks/826-e0e455fb469c158f.js" defer=""></script><script src="/DevNotes/_next/static/chunks/986-737e5da213076068.js" defer=""></script><script src="/DevNotes/_next/static/chunks/pages/notes/%5Bid%5D-00dd1421f3ce3a3e.js" defer=""></script><script src="/DevNotes/_next/static/ZngNBAAOWdGj2M_dMvVk6/_buildManifest.js" defer=""></script><script src="/DevNotes/_next/static/ZngNBAAOWdGj2M_dMvVk6/_ssgManifest.js" defer=""></script><script src="/DevNotes/_next/static/ZngNBAAOWdGj2M_dMvVk6/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="computer-vision"><a aria-hidden="true" class="anchor-heading" href="#computer-vision"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Computer Vision</h1>
<h2 id="computer-vision-models-and-capabilities"><a aria-hidden="true" class="anchor-heading" href="#computer-vision-models-and-capabilities"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Computer Vision Models and Capabilities</h2>
<ul>
<li>
<p>Image Classification</p>
</li>
<li>
<p>Object Detection</p>
</li>
<li>
<p>Semantic Segmentation - Semantic segmentation is an advanced machine learning technique in which individual pixels in the image are classified according to the object to which they belong.</p>
</li>
<li>
<p>Image Analysis - You can create solutions that combine machine learning models with advanced image analysis techniques to extract information from images, including "tags" that could help catalog the image or even descriptive captions that summarize the scene shown in the image.</p>
</li>
<li>
<p>Face detection, analysis and recognition</p>
</li>
<li>
<p>Optical Character Recognition - is a technique used to detect and read text in images. You can use OCR to read text in photographs (for example, road signs or store fronts) or to extract information from scanned documents such as letters, invoices, or forms.</p>
</li>
</ul>
<h2 id="computer-vision-services-in-microsoft-azure"><a aria-hidden="true" class="anchor-heading" href="#computer-vision-services-in-microsoft-azure"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Computer vision services in Microsoft Azure</h2>
<table aria-label="Computer vision services in Microsoft Azure" class="table">
<thead>
<tr>
<th>Service</th>
<th>Capabilities</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Computer Vision</strong></td>
<td>You can use this service to analyze images and video, and extract descriptions, tags, objects, and text.</td>
</tr>
<tr>
<td><strong>Custom Vision</strong></td>
<td>Use this service to train custom image classification and object detection models using your own images.</td>
</tr>
<tr>
<td><strong>Face</strong></td>
<td>The Face service enables you to build face detection and facial recognition solutions.</td>
</tr>
<tr>
<td><strong>Form Recognizer</strong></td>
<td>Use this service to extract information from scanned forms and invoices.</td>
</tr>
</tbody>
</table>
<h2 id="analyzing-images-with-the-cv-service"><a aria-hidden="true" class="anchor-heading" href="#analyzing-images-with-the-cv-service"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Analyzing images with the CV Service</h2>
<h3 id="describing-an-image"><a aria-hidden="true" class="anchor-heading" href="#describing-an-image"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Describing an image</h3>
<p>Computer Vision has the ability to analyze an image, evaluate the objects that are detected, and generate a human-readable phrase or sentence that can describe what was detected in the image. Depending on the image contents, the service may return multiple results, or phrases. Each returned phrase will have an associated confidence score, indicating how confident the algorithm is in the supplied description. The highest confidence phrases will be listed first.</p>
<p>To help you understand this concept, consider the following image of the Empire State building in New York. The returned phrases are listed below the image in the order of confidence.</p>
<p><img src="/DevNotes/assets/black-white-buildings.png" alt="NYC Skyline"></p>
<ul>
<li>A black and white photo of a city</li>
<li>A black and white photo of a large city</li>
<li>A large white building in a city</li>
</ul>
<h3 id="tagging-visual-features"><a aria-hidden="true" class="anchor-heading" href="#tagging-visual-features"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Tagging visual features</h3>
<p>The image descriptions generated by Computer Vision are based on a set of thousands of recognizable objects, which can be used to suggest tags for the image. These tags can be associated with the image as metadata that summarizes attributes of the image; and can be particularly useful if you want to index an image along with a set of key terms that might be used to search for images with specific attributes or contents.</p>
<p>For example, the tags returned for the Empire State building image include:</p>
<ul>
<li>skyscraper</li>
<li>tower</li>
<li>building</li>
</ul>
<h3 id="detecting-objects"><a aria-hidden="true" class="anchor-heading" href="#detecting-objects"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Detecting objects</h3>
<p>The object detection capability is similar to tagging, in that the service can identify common objects; but rather than tagging, or providing tags for the recognized objects only, this service can also return what is known as bounding box coordinates. Not only will you get the type of object, but you will also receive a set of coordinates that indicate the top, left, width, and height of the object detected, which you can use to identify the location of the object in the image, like this:</p>
<p><img src="/DevNotes/assets/black-white-buildings-objects.png" alt="NYC Skyline Objects Highlighted"></p>
<h3 id="detecting-brands"><a aria-hidden="true" class="anchor-heading" href="#detecting-brands"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Detecting brands</h3>
<p>This feature provides the ability to identify commercial brands. The service has an existing database of thousands of globally recognized logos from commercial brands of products.</p>
<p>When you call the service and pass it an image, it performs a detection task and determine if any of the identified objects in the image are recognized brands. The service compares the brands against its database of popular brands spanning clothing, consumer electronics, and many more categories. If a known brand is detected, the service returns a response that contains the brand name, a confidence score (from 0 to 1 indicating how positive the identification is), and a bounding box (coordinates) for where in the image the detected brand was found.</p>
<p>For example, in the following image, a laptop has a Microsoft logo on its lid, which is identified and located by the Computer Vision service.</p>
<p><img src="/DevNotes/assets/laptop.png" alt="Laptop Brand"></p>
<h3 id="detecting-faces"><a aria-hidden="true" class="anchor-heading" href="#detecting-faces"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Detecting faces</h3>
<p>The Computer Vision service can detect and analyze human faces in an image, including the ability to determine age and a bounding box rectangle for the location of the face(s). The facial analysis capabilities of the Computer Vision service are a subset of those provided by the dedicated Face Service. If you need basic face detection and analysis, combined with general image analysis capabilities, you can use the Computer Vision service; but for more comprehensive facial analysis and facial recognition functionality, use the Face service.</p>
<p>The following example shows an image of a person with their face detected and approximate age estimated.</p>
<p><img src="/DevNotes/assets/face.png" alt="Face Recognition"></p>
<h3 id="categorizing-an-image"><a aria-hidden="true" class="anchor-heading" href="#categorizing-an-image"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Categorizing an image</h3>
<p>Computer Vision can categorize images based on their contents. The service uses a parent/child hierarchy with a "current" limited set of categories. When analyzing an image, detected objects are compared to the existing categories to determine the best way to provide the categorization. As an example, one of the parent categories is <code>people_</code>. This image of a person on a roof is assigned a category of <code>people_</code>.</p>
<h3 id="computer-vision-86-catergory-taxonomy"><a aria-hidden="true" class="anchor-heading" href="#computer-vision-86-catergory-taxonomy"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Computer Vision 86 Catergory Taxonomy</h3>
<p><a href="https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/category-taxonomy">Link to Microsoft Docs List</a></p>
<h3 id="detecting-domain-specific-content"><a aria-hidden="true" class="anchor-heading" href="#detecting-domain-specific-content"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Detecting domain-specific content</h3>
<p>When categorizing an image, the Computer Vision service supports two specialized domain models:</p>
<ul>
<li><strong>Celebrities</strong> - The service includes a model that has been trained to identify thousands of well-known celebrities from the worlds of sports, entertainment, and business.</li>
<li><strong>Landmarks</strong> - The service can identify famous landmarks, such as the Taj Mahal and the Statue of Liberty.</li>
</ul>
<p>For example, when analyzing the following image for landmarks, the Computer Vision service identifies the Eiffel Tower, with a confidence of 99.41%.</p>
<h3 id="optical-character-recognition"><a aria-hidden="true" class="anchor-heading" href="#optical-character-recognition"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Optical character recognition</h3>
<p>The Computer Vision service can use optical character recognition (OCR) capabilities to detect printed and handwritten text in images. This capability is explored in the Read text with the Computer Vision service module on Microsoft Learn.</p>
<h3 id="additional-capabilities"><a aria-hidden="true" class="anchor-heading" href="#additional-capabilities"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Additional capabilities</h3>
<p>In addition to these capabilities, the Computer Vision service can:</p>
<ul>
<li><strong>Detect image types</strong> - for example, identifying clip art images or line drawings.</li>
<li><strong>Detect image color schemes</strong> - specifically, identifying the dominant foreground, background, and overall colors in an image.</li>
<li><strong>Generate thumbnails</strong> - creating small versions of images.</li>
<li><strong>Moderate content</strong> - detecting images that contain adult content or depict violent, gory scenes.</li>
</ul>
<h2 id="classifying-images-with-cv"><a aria-hidden="true" class="anchor-heading" href="#classifying-images-with-cv"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Classifying Images with CV</h2>
<p>Image classification is a machine learning technique in which the object being classified is an image, such as a photograph.</p>
<p>To create an image classification model, you need data that consists of features and their labels. The existing data is a set of categorized images. Digital images are made up of an array of pixel values, and these are used as features to train the model based on the known image classes.</p>
<p>You can use a machine learning classification technique to predict which category, or class, something belongs to. Classification machine learning models use a set of inputs, which we call features, to calculate a probability score for each possible class and predict a label that indicates the most likely class that an object belongs to.</p>
<p>The model is trained to match the patterns in the pixel values to a set of class labels. After the model has been trained, you can use it with new sets of features to predict unknown label values.</p>
<p>Most modern image classification solutions are based on deep learning techniques that make use of convolutional neural networks (CNNs) to uncover patterns in the pixels that correspond to particular classes. Training an effective CNN is a complex task that requires considerable expertise in data science and machine learning.</p>
<h3 id="model-evaluation"><a aria-hidden="true" class="anchor-heading" href="#model-evaluation"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Model Evaluation</h3>
<p>Model training process is an iterative process in which the Custom Vision service repeatedly trains the model using some of the data, but holds some back to evaluate the model. At the end of the training process, the performance for the trained model is indicated by the following evaluation metrics:</p>
<ul>
<li><strong>Precision</strong>: What percentage of the class predictions made by the model were correct? For example, if the model predicted that 10 images are oranges, of which eight were actually oranges, then the precision is 0.8 (80%).</li>
<li><strong>Recall</strong>: What percentage of class predictions did the model correctly identify? For example, if there are 10 images of apples, and the model found 7 of them, then the recall is 0.7 (70%).</li>
<li><strong>Average Precision (AP)</strong>: An overall metric that takes into account both precision and recall).</li>
</ul>
<h2 id="object-detection"><a aria-hidden="true" class="anchor-heading" href="#object-detection"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Object Detection</h2>
<p>Object detection is a form of machine learning based computer vision in which a model is trained to recognize individual types of objects in an image, and to identify their location in the image.</p>
<h3 id="object-detection-vs-image-classification"><a aria-hidden="true" class="anchor-heading" href="#object-detection-vs-image-classification"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Object detection vs. image classification</h3>
<p>Image classification is a machine learning based form of computer vision in which a model is trained to categorize images based on the primary subject matter they contain. Object detection goes further than this to classify individual objects within the image, and to return the coordinates of a bounding box that indicates the object's location.</p>
<h3 id="image-tagging"><a aria-hidden="true" class="anchor-heading" href="#image-tagging"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Image Tagging</h3>
<p>Before you can train an object detection model, you must tag the classes and bounding box coordinates in a set of training images.</p>
<h3 id="model-training-and-evaluation"><a aria-hidden="true" class="anchor-heading" href="#model-training-and-evaluation"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Model training and evaluation</h3>
<ul>
<li><strong>Precision</strong>: What percentage of class predictions did the model correctly identify? For example, if the model predicted that 10 images are oranges, of which eight were actually oranges, then the precision is 0.8 (80%).</li>
<li><strong>Recall</strong>: What percentage of the class predictions made by the model were correct? For example, if there are 10 images of apples, and the model found 7 of them, then the recall is 0.7 (70%).</li>
<li><strong>Mean Average Precision (mAP)</strong>: An overall metric that takes into account both precision and recall across all classes).</li>
</ul>
<h2 id="face-detection"><a aria-hidden="true" class="anchor-heading" href="#face-detection"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Face Detection</h2>
<p>Face detection involves identifying regions of an image that contain a human face, typically by returning bounding box coordinates that form a rectangle around the face, like this:</p>
<p><img src="/DevNotes/assets/face-detection.png" alt="Face Detection"></p>
<h3 id="facial-analysis"><a aria-hidden="true" class="anchor-heading" href="#facial-analysis"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Facial Analysis</h3>
<p>Moving beyond simple face detection, some algorithms can also return other information, such as facial landmarks (nose, eyes, eyebrows, lips, and others).</p>
<p><img src="/DevNotes/assets/facial-landmarks.png" alt="Facial Landmarks"></p>
<p>hese facial landmarks can be used as features with which to train a machine learning model from which you can infer information about a person, such as their perceived age or perceived emotional state.</p>
<h3 id="facial-recognition"><a aria-hidden="true" class="anchor-heading" href="#facial-recognition"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Facial recognition</h3>
<p>A further application of facial analysis is to train a machine learning model to identify known individuals from their facial features. This usage is more generally known as facial recognition, and involves using multiple images of each person you want to recognize to train a model so that it can detect those individuals in new images on which it wasn't trained.</p>
<h3 id="use-cases-for-face-detection-and-analysis"><a aria-hidden="true" class="anchor-heading" href="#use-cases-for-face-detection-and-analysis"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Use Cases for Face Detection and Analysis</h3>
<ul>
<li><strong>Security</strong> - facial recognition can be used in building security applications, and increasingly it is used in smart phones operating systems for unlocking devices.</li>
<li><strong>Social media</strong> - facial recognition can be used to automatically tag known friends in photographs.</li>
<li><strong>Intelligent monitoring</strong> - for example, an automobile might include a system that monitors the driver's face to determine if the driver is looking at the road, looking at a mobile device, or shows signs of tiredness.</li>
<li><strong>Advertising</strong> - analyzing faces in an image can help direct advertisements to an appropriate demographic audience.</li>
<li><strong>Missing persons</strong> - using public cameras systems, facial recognition can be used to identify if a missing person is in the image frame.</li>
<li><strong>Identity validation</strong> - useful at ports of entry kiosks where a person holds a special entry permit.</li>
</ul>
<h2 id="text-detection"><a aria-hidden="true" class="anchor-heading" href="#text-detection"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Text Detection</h2>
<h3 id="optical-character-recognition-ocr"><a aria-hidden="true" class="anchor-heading" href="#optical-character-recognition-ocr"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Optical Character Recognition (OCR)</h3>
<p>The basic foundation of processing printed text is optical character recognition (OCR), in which a model can be trained to recognize individual shapes as letters, numerals, punctuation, or other elements of text. Much of the early work on implementing this kind of capability was performed by postal services to support automatic sorting of mail based on postal codes. Since then, the state-of-the-art for reading text has moved on, and it's now possible to build models that can detect printed or handwritten text in an image and read it line-by-line or even word-by-word.</p>
<h3 id="machine-reading-comprehension-mrc"><a aria-hidden="true" class="anchor-heading" href="#machine-reading-comprehension-mrc"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Machine Reading Comprehension (MRC)</h3>
<p>At the other end of the scale, there is machine reading comprehension (MRC), in which an AI system not only reads the text characters, but can use a semantic model to interpret what the text is about.</p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#computer-vision-models-and-capabilities" title="Computer Vision Models and Capabilities">Computer Vision Models and Capabilities</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#computer-vision-services-in-microsoft-azure" title="Computer vision services in Microsoft Azure">Computer vision services in Microsoft Azure</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#analyzing-images-with-the-cv-service" title="Analyzing images with the CV Service">Analyzing images with the CV Service</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#describing-an-image" title="Describing an image">Describing an image</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tagging-visual-features" title="Tagging visual features">Tagging visual features</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#detecting-objects" title="Detecting objects">Detecting objects</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#detecting-brands" title="Detecting brands">Detecting brands</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#detecting-faces" title="Detecting faces">Detecting faces</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#categorizing-an-image" title="Categorizing an image">Categorizing an image</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#computer-vision-86-catergory-taxonomy" title="Computer Vision 86 Catergory Taxonomy">Computer Vision 86 Catergory Taxonomy</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#detecting-domain-specific-content" title="Detecting domain-specific content">Detecting domain-specific content</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#optical-character-recognition" title="Optical character recognition">Optical character recognition</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#additional-capabilities" title="Additional capabilities">Additional capabilities</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#classifying-images-with-cv" title="Classifying Images with CV">Classifying Images with CV</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#model-evaluation" title="Model Evaluation">Model Evaluation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#object-detection" title="Object Detection">Object Detection</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#object-detection-vs-image-classification" title="Object detection vs. image classification">Object detection vs. image classification</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#image-tagging" title="Image Tagging">Image Tagging</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#model-training-and-evaluation" title="Model training and evaluation">Model training and evaluation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#face-detection" title="Face Detection">Face Detection</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#facial-analysis" title="Facial Analysis">Facial Analysis</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#facial-recognition" title="Facial recognition">Facial recognition</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#use-cases-for-face-detection-and-analysis" title="Use Cases for Face Detection and Analysis">Use Cases for Face Detection and Analysis</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#text-detection" title="Text Detection">Text Detection</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#optical-character-recognition-ocr" title="Optical Character Recognition (OCR)">Optical Character Recognition (OCR)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#machine-reading-comprehension-mrc" title="Machine Reading Comprehension (MRC)">Machine Reading Comprehension (MRC)</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"g1he2uw6667t294j66dgt4k","title":"Computer Vision","desc":"Notes on Azure AI Computer Vision","updated":1649876155601,"created":1648504410812,"custom":{},"fname":"devnotes.azure.ai.computer-vision","type":"note","vault":{"fsPath":"vault"},"contentHash":"c8cb49901d621a26f809090aca6daa3a","links":[],"anchors":{"computer-vision-models-and-capabilities":{"type":"header","text":"Computer Vision Models and Capabilities","value":"computer-vision-models-and-capabilities","line":7,"column":0,"depth":2},"computer-vision-services-in-microsoft-azure":{"type":"header","text":"Computer vision services in Microsoft Azure","value":"computer-vision-services-in-microsoft-azure","line":21,"column":0,"depth":2},"analyzing-images-with-the-cv-service":{"type":"header","text":"Analyzing images with the CV Service","value":"analyzing-images-with-the-cv-service","line":50,"column":0,"depth":2},"describing-an-image":{"type":"header","text":"Describing an image","value":"describing-an-image","line":52,"column":0,"depth":3},"tagging-visual-features":{"type":"header","text":"Tagging visual features","value":"tagging-visual-features","line":64,"column":0,"depth":3},"detecting-objects":{"type":"header","text":"Detecting objects","value":"detecting-objects","line":74,"column":0,"depth":3},"detecting-brands":{"type":"header","text":"Detecting brands","value":"detecting-brands","line":80,"column":0,"depth":3},"detecting-faces":{"type":"header","text":"Detecting faces","value":"detecting-faces","line":90,"column":0,"depth":3},"categorizing-an-image":{"type":"header","text":"Categorizing an image","value":"categorizing-an-image","line":98,"column":0,"depth":3},"computer-vision-86-catergory-taxonomy":{"type":"header","text":"Computer Vision 86 Catergory Taxonomy","value":"computer-vision-86-catergory-taxonomy","line":102,"column":0,"depth":3},"detecting-domain-specific-content":{"type":"header","text":"Detecting domain-specific content","value":"detecting-domain-specific-content","line":106,"column":0,"depth":3},"optical-character-recognition":{"type":"header","text":"Optical character recognition","value":"optical-character-recognition","line":115,"column":0,"depth":3},"additional-capabilities":{"type":"header","text":"Additional capabilities","value":"additional-capabilities","line":119,"column":0,"depth":3},"classifying-images-with-cv":{"type":"header","text":"Classifying Images with CV","value":"classifying-images-with-cv","line":128,"column":0,"depth":2},"model-evaluation":{"type":"header","text":"Model Evaluation","value":"model-evaluation","line":140,"column":0,"depth":3},"object-detection":{"type":"header","text":"Object Detection","value":"object-detection","line":148,"column":0,"depth":2},"object-detection-vs-image-classification":{"type":"header","text":"Object detection vs. image classification","value":"object-detection-vs-image-classification","line":152,"column":0,"depth":3},"image-tagging":{"type":"header","text":"Image Tagging","value":"image-tagging","line":156,"column":0,"depth":3},"model-training-and-evaluation":{"type":"header","text":"Model training and evaluation","value":"model-training-and-evaluation","line":160,"column":0,"depth":3},"face-detection":{"type":"header","text":"Face Detection","value":"face-detection","line":166,"column":0,"depth":2},"facial-analysis":{"type":"header","text":"Facial Analysis","value":"facial-analysis","line":172,"column":0,"depth":3},"facial-recognition":{"type":"header","text":"Facial recognition","value":"facial-recognition","line":180,"column":0,"depth":3},"use-cases-for-face-detection-and-analysis":{"type":"header","text":"Use Cases for Face Detection and Analysis","value":"use-cases-for-face-detection-and-analysis","line":184,"column":0,"depth":3},"text-detection":{"type":"header","text":"Text Detection","value":"text-detection","line":193,"column":0,"depth":2},"optical-character-recognition-ocr":{"type":"header","text":"Optical Character Recognition (OCR)","value":"optical-character-recognition-ocr","line":195,"column":0,"depth":3},"machine-reading-comprehension-mrc":{"type":"header","text":"Machine Reading Comprehension (MRC)","value":"machine-reading-comprehension-mrc","line":199,"column":0,"depth":3}},"children":[],"parent":"bxv1vlc2jzbzpchawt74rzq","data":{}},"body":"\u003ch1 id=\"computer-vision\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eComputer Vision\u003c/h1\u003e\n\u003ch2 id=\"computer-vision-models-and-capabilities\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision-models-and-capabilities\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eComputer Vision Models and Capabilities\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eImage Classification\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eObject Detection\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSemantic Segmentation - Semantic segmentation is an advanced machine learning technique in which individual pixels in the image are classified according to the object to which they belong.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImage Analysis - You can create solutions that combine machine learning models with advanced image analysis techniques to extract information from images, including \"tags\" that could help catalog the image or even descriptive captions that summarize the scene shown in the image.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFace detection, analysis and recognition\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOptical Character Recognition - is a technique used to detect and read text in images. You can use OCR to read text in photographs (for example, road signs or store fronts) or to extract information from scanned documents such as letters, invoices, or forms.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"computer-vision-services-in-microsoft-azure\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision-services-in-microsoft-azure\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eComputer vision services in Microsoft Azure\u003c/h2\u003e\n\u003ctable aria-label=\"Computer vision services in Microsoft Azure\" class=\"table\"\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eService\u003c/th\u003e\n\u003cth\u003eCapabilities\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eComputer Vision\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eYou can use this service to analyze images and video, and extract descriptions, tags, objects, and text.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eCustom Vision\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eUse this service to train custom image classification and object detection models using your own images.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eFace\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eThe Face service enables you to build face detection and facial recognition solutions.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eForm Recognizer\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eUse this service to extract information from scanned forms and invoices.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"analyzing-images-with-the-cv-service\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#analyzing-images-with-the-cv-service\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eAnalyzing images with the CV Service\u003c/h2\u003e\n\u003ch3 id=\"describing-an-image\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#describing-an-image\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDescribing an image\u003c/h3\u003e\n\u003cp\u003eComputer Vision has the ability to analyze an image, evaluate the objects that are detected, and generate a human-readable phrase or sentence that can describe what was detected in the image. Depending on the image contents, the service may return multiple results, or phrases. Each returned phrase will have an associated confidence score, indicating how confident the algorithm is in the supplied description. The highest confidence phrases will be listed first.\u003c/p\u003e\n\u003cp\u003eTo help you understand this concept, consider the following image of the Empire State building in New York. The returned phrases are listed below the image in the order of confidence.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/DevNotes/assets/black-white-buildings.png\" alt=\"NYC Skyline\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA black and white photo of a city\u003c/li\u003e\n\u003cli\u003eA black and white photo of a large city\u003c/li\u003e\n\u003cli\u003eA large white building in a city\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tagging-visual-features\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#tagging-visual-features\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eTagging visual features\u003c/h3\u003e\n\u003cp\u003eThe image descriptions generated by Computer Vision are based on a set of thousands of recognizable objects, which can be used to suggest tags for the image. These tags can be associated with the image as metadata that summarizes attributes of the image; and can be particularly useful if you want to index an image along with a set of key terms that might be used to search for images with specific attributes or contents.\u003c/p\u003e\n\u003cp\u003eFor example, the tags returned for the Empire State building image include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eskyscraper\u003c/li\u003e\n\u003cli\u003etower\u003c/li\u003e\n\u003cli\u003ebuilding\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"detecting-objects\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-objects\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDetecting objects\u003c/h3\u003e\n\u003cp\u003eThe object detection capability is similar to tagging, in that the service can identify common objects; but rather than tagging, or providing tags for the recognized objects only, this service can also return what is known as bounding box coordinates. Not only will you get the type of object, but you will also receive a set of coordinates that indicate the top, left, width, and height of the object detected, which you can use to identify the location of the object in the image, like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/DevNotes/assets/black-white-buildings-objects.png\" alt=\"NYC Skyline Objects Highlighted\"\u003e\u003c/p\u003e\n\u003ch3 id=\"detecting-brands\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-brands\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDetecting brands\u003c/h3\u003e\n\u003cp\u003eThis feature provides the ability to identify commercial brands. The service has an existing database of thousands of globally recognized logos from commercial brands of products.\u003c/p\u003e\n\u003cp\u003eWhen you call the service and pass it an image, it performs a detection task and determine if any of the identified objects in the image are recognized brands. The service compares the brands against its database of popular brands spanning clothing, consumer electronics, and many more categories. If a known brand is detected, the service returns a response that contains the brand name, a confidence score (from 0 to 1 indicating how positive the identification is), and a bounding box (coordinates) for where in the image the detected brand was found.\u003c/p\u003e\n\u003cp\u003eFor example, in the following image, a laptop has a Microsoft logo on its lid, which is identified and located by the Computer Vision service.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/DevNotes/assets/laptop.png\" alt=\"Laptop Brand\"\u003e\u003c/p\u003e\n\u003ch3 id=\"detecting-faces\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-faces\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDetecting faces\u003c/h3\u003e\n\u003cp\u003eThe Computer Vision service can detect and analyze human faces in an image, including the ability to determine age and a bounding box rectangle for the location of the face(s). The facial analysis capabilities of the Computer Vision service are a subset of those provided by the dedicated Face Service. If you need basic face detection and analysis, combined with general image analysis capabilities, you can use the Computer Vision service; but for more comprehensive facial analysis and facial recognition functionality, use the Face service.\u003c/p\u003e\n\u003cp\u003eThe following example shows an image of a person with their face detected and approximate age estimated.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/DevNotes/assets/face.png\" alt=\"Face Recognition\"\u003e\u003c/p\u003e\n\u003ch3 id=\"categorizing-an-image\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#categorizing-an-image\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eCategorizing an image\u003c/h3\u003e\n\u003cp\u003eComputer Vision can categorize images based on their contents. The service uses a parent/child hierarchy with a \"current\" limited set of categories. When analyzing an image, detected objects are compared to the existing categories to determine the best way to provide the categorization. As an example, one of the parent categories is \u003ccode\u003epeople_\u003c/code\u003e. This image of a person on a roof is assigned a category of \u003ccode\u003epeople_\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"computer-vision-86-catergory-taxonomy\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#computer-vision-86-catergory-taxonomy\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eComputer Vision 86 Catergory Taxonomy\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/category-taxonomy\"\u003eLink to Microsoft Docs List\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"detecting-domain-specific-content\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#detecting-domain-specific-content\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eDetecting domain-specific content\u003c/h3\u003e\n\u003cp\u003eWhen categorizing an image, the Computer Vision service supports two specialized domain models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCelebrities\u003c/strong\u003e - The service includes a model that has been trained to identify thousands of well-known celebrities from the worlds of sports, entertainment, and business.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLandmarks\u003c/strong\u003e - The service can identify famous landmarks, such as the Taj Mahal and the Statue of Liberty.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, when analyzing the following image for landmarks, the Computer Vision service identifies the Eiffel Tower, with a confidence of 99.41%.\u003c/p\u003e\n\u003ch3 id=\"optical-character-recognition\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#optical-character-recognition\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eOptical character recognition\u003c/h3\u003e\n\u003cp\u003eThe Computer Vision service can use optical character recognition (OCR) capabilities to detect printed and handwritten text in images. This capability is explored in the Read text with the Computer Vision service module on Microsoft Learn.\u003c/p\u003e\n\u003ch3 id=\"additional-capabilities\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#additional-capabilities\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eAdditional capabilities\u003c/h3\u003e\n\u003cp\u003eIn addition to these capabilities, the Computer Vision service can:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDetect image types\u003c/strong\u003e - for example, identifying clip art images or line drawings.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDetect image color schemes\u003c/strong\u003e - specifically, identifying the dominant foreground, background, and overall colors in an image.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGenerate thumbnails\u003c/strong\u003e - creating small versions of images.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModerate content\u003c/strong\u003e - detecting images that contain adult content or depict violent, gory scenes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"classifying-images-with-cv\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#classifying-images-with-cv\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eClassifying Images with CV\u003c/h2\u003e\n\u003cp\u003eImage classification is a machine learning technique in which the object being classified is an image, such as a photograph.\u003c/p\u003e\n\u003cp\u003eTo create an image classification model, you need data that consists of features and their labels. The existing data is a set of categorized images. Digital images are made up of an array of pixel values, and these are used as features to train the model based on the known image classes.\u003c/p\u003e\n\u003cp\u003eYou can use a machine learning classification technique to predict which category, or class, something belongs to. Classification machine learning models use a set of inputs, which we call features, to calculate a probability score for each possible class and predict a label that indicates the most likely class that an object belongs to.\u003c/p\u003e\n\u003cp\u003eThe model is trained to match the patterns in the pixel values to a set of class labels. After the model has been trained, you can use it with new sets of features to predict unknown label values.\u003c/p\u003e\n\u003cp\u003eMost modern image classification solutions are based on deep learning techniques that make use of convolutional neural networks (CNNs) to uncover patterns in the pixels that correspond to particular classes. Training an effective CNN is a complex task that requires considerable expertise in data science and machine learning.\u003c/p\u003e\n\u003ch3 id=\"model-evaluation\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#model-evaluation\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eModel Evaluation\u003c/h3\u003e\n\u003cp\u003eModel training process is an iterative process in which the Custom Vision service repeatedly trains the model using some of the data, but holds some back to evaluate the model. At the end of the training process, the performance for the trained model is indicated by the following evaluation metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrecision\u003c/strong\u003e: What percentage of the class predictions made by the model were correct? For example, if the model predicted that 10 images are oranges, of which eight were actually oranges, then the precision is 0.8 (80%).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecall\u003c/strong\u003e: What percentage of class predictions did the model correctly identify? For example, if there are 10 images of apples, and the model found 7 of them, then the recall is 0.7 (70%).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAverage Precision (AP)\u003c/strong\u003e: An overall metric that takes into account both precision and recall).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"object-detection\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#object-detection\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eObject Detection\u003c/h2\u003e\n\u003cp\u003eObject detection is a form of machine learning based computer vision in which a model is trained to recognize individual types of objects in an image, and to identify their location in the image.\u003c/p\u003e\n\u003ch3 id=\"object-detection-vs-image-classification\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#object-detection-vs-image-classification\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eObject detection vs. image classification\u003c/h3\u003e\n\u003cp\u003eImage classification is a machine learning based form of computer vision in which a model is trained to categorize images based on the primary subject matter they contain. Object detection goes further than this to classify individual objects within the image, and to return the coordinates of a bounding box that indicates the object's location.\u003c/p\u003e\n\u003ch3 id=\"image-tagging\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#image-tagging\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eImage Tagging\u003c/h3\u003e\n\u003cp\u003eBefore you can train an object detection model, you must tag the classes and bounding box coordinates in a set of training images.\u003c/p\u003e\n\u003ch3 id=\"model-training-and-evaluation\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#model-training-and-evaluation\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eModel training and evaluation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePrecision\u003c/strong\u003e: What percentage of class predictions did the model correctly identify? For example, if the model predicted that 10 images are oranges, of which eight were actually oranges, then the precision is 0.8 (80%).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecall\u003c/strong\u003e: What percentage of the class predictions made by the model were correct? For example, if there are 10 images of apples, and the model found 7 of them, then the recall is 0.7 (70%).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMean Average Precision (mAP)\u003c/strong\u003e: An overall metric that takes into account both precision and recall across all classes).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"face-detection\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#face-detection\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eFace Detection\u003c/h2\u003e\n\u003cp\u003eFace detection involves identifying regions of an image that contain a human face, typically by returning bounding box coordinates that form a rectangle around the face, like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/DevNotes/assets/face-detection.png\" alt=\"Face Detection\"\u003e\u003c/p\u003e\n\u003ch3 id=\"facial-analysis\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#facial-analysis\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eFacial Analysis\u003c/h3\u003e\n\u003cp\u003eMoving beyond simple face detection, some algorithms can also return other information, such as facial landmarks (nose, eyes, eyebrows, lips, and others).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/DevNotes/assets/facial-landmarks.png\" alt=\"Facial Landmarks\"\u003e\u003c/p\u003e\n\u003cp\u003ehese facial landmarks can be used as features with which to train a machine learning model from which you can infer information about a person, such as their perceived age or perceived emotional state.\u003c/p\u003e\n\u003ch3 id=\"facial-recognition\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#facial-recognition\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eFacial recognition\u003c/h3\u003e\n\u003cp\u003eA further application of facial analysis is to train a machine learning model to identify known individuals from their facial features. This usage is more generally known as facial recognition, and involves using multiple images of each person you want to recognize to train a model so that it can detect those individuals in new images on which it wasn't trained.\u003c/p\u003e\n\u003ch3 id=\"use-cases-for-face-detection-and-analysis\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#use-cases-for-face-detection-and-analysis\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eUse Cases for Face Detection and Analysis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSecurity\u003c/strong\u003e - facial recognition can be used in building security applications, and increasingly it is used in smart phones operating systems for unlocking devices.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSocial media\u003c/strong\u003e - facial recognition can be used to automatically tag known friends in photographs.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntelligent monitoring\u003c/strong\u003e - for example, an automobile might include a system that monitors the driver's face to determine if the driver is looking at the road, looking at a mobile device, or shows signs of tiredness.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAdvertising\u003c/strong\u003e - analyzing faces in an image can help direct advertisements to an appropriate demographic audience.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMissing persons\u003c/strong\u003e - using public cameras systems, facial recognition can be used to identify if a missing person is in the image frame.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIdentity validation\u003c/strong\u003e - useful at ports of entry kiosks where a person holds a special entry permit.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"text-detection\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#text-detection\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eText Detection\u003c/h2\u003e\n\u003ch3 id=\"optical-character-recognition-ocr\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#optical-character-recognition-ocr\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eOptical Character Recognition (OCR)\u003c/h3\u003e\n\u003cp\u003eThe basic foundation of processing printed text is optical character recognition (OCR), in which a model can be trained to recognize individual shapes as letters, numerals, punctuation, or other elements of text. Much of the early work on implementing this kind of capability was performed by postal services to support automatic sorting of mail based on postal codes. Since then, the state-of-the-art for reading text has moved on, and it's now possible to build models that can detect printed or handwritten text in an image and read it line-by-line or even word-by-word.\u003c/p\u003e\n\u003ch3 id=\"machine-reading-comprehension-mrc\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#machine-reading-comprehension-mrc\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eMachine Reading Comprehension (MRC)\u003c/h3\u003e\n\u003cp\u003eAt the other end of the scale, there is machine reading comprehension (MRC), in which an AI system not only reads the text characters, but can use a semantic model to interpret what the text is about.\u003c/p\u003e","noteIndex":{"id":"root","title":"root","desc":"","updated":1605266684036,"created":1595961348801,"stub":false,"custom":{"stub":false,"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"cf4622cd4e7841f93fb3051e6203c8e6","links":[],"anchors":{},"children":["uRNDWirv66xgHSdrPwMcw","lcTujcWsxBfPEpzP49YbQ","FTV8hEHhyid0DiEvv9gNj","ph4ta0hoi7x0vch4uoz72hn","boufkx25rro7ysodpanan5g","01u0co3RYjOM1bjpc2qIU","xt9womgr90xi5ui5nezev7b","qpngd6ap92mmahr4bh4gc47","hobifg8vvmmg17kt2z5p36a"],"parent":null,"data":{},"body":"\nThis is the root for your Dendron vault.\n\nIf you decide to publish your entire vault, it will be your landing page. You are free to customize any part of this page except the frontmatter at the top, between the `---`. \n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"dendronVersion":"0.88.0","enableFullHierarchyNoteTitle":false,"enableHandlebarTemplates":true,"enableSmartRefs":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"assetsPrefix":"/DevNotes","copyAssets":true,"siteHierarchies":["root"],"enableSiteLastModified":true,"siteRootDir":"docs","siteUrl":"https://rperez2021.github.io","enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"writeStubs":false,"seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"root","enableTaskNotes":true}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"g1he2uw6667t294j66dgt4k"},"buildId":"ZngNBAAOWdGj2M_dMvVk6","assetPrefix":"/DevNotes","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>